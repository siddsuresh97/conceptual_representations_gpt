{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from src.preprocess import *\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(sentence_batches, checker, corrector, separator: str = \" \"):\n",
    "    \"\"\"\n",
    "    Correct the grammar in a string of text using a text-classification and text-generation pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be corrected.\n",
    "    checker (transformers.pipeline.Pipeline): The text-classification pipeline to use for checking the grammar quality of the text.\n",
    "    corrector (transformers.pipeline.Pipeline): The text-generation pipeline to use for correcting the text.\n",
    "    separator (str, optional): The separator to use when joining the corrected text into a single string. Default is a space character.\n",
    "\n",
    "    Returns:\n",
    "    str: The corrected text.\n",
    "    \"\"\"\n",
    "    # # Split the text into sentence batches\n",
    "    # sentence_batches = split_text(text)\n",
    "\n",
    "    # Initialize a list to store the corrected text\n",
    "    corrected_text = []\n",
    "\n",
    "    # Iterate through the sentence batches\n",
    "    for batch in tqdm(\n",
    "        sentence_batches, total=len(sentence_batches), desc=\"correcting text..\"\n",
    "    ):\n",
    "        # Join the sentences in the batch into a single string\n",
    "        raw_text = \" \".join(batch)\n",
    "\n",
    "        # Check the grammar quality of the text using the text-classification pipeline\n",
    "        results = checker(raw_text)\n",
    "\n",
    "        # Only correct the text if the results of the text-classification are not LABEL_1 or are LABEL_1 with a score below 0.9\n",
    "        if results[0][\"label\"] != \"LABEL_1\" or (\n",
    "            results[0][\"label\"] == \"LABEL_1\" and results[0][\"score\"] < 0.9\n",
    "        ):\n",
    "            # Correct the text using the text-generation pipeline\n",
    "            corrected_batch = corrector(raw_text)\n",
    "            corrected_text.append(corrected_batch[0][\"generated_text\"])\n",
    "        else:\n",
    "            corrected_text.append(raw_text)\n",
    "\n",
    "    # Join the corrected text into a single string\n",
    "    # corrected_text = separator.join(corrected_text)\n",
    "\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_leuven_prompt(concept, feature, batches_with_prompts):\n",
    "    '''This function adds the prompts to the batches.'''\n",
    "    # prompt = \"Help me write a prompt as a question from a concept and an attribute. \\nConcept: {}\\nAttribute: {}.\\nPrompt: In one word Yes/No <mask> ?\".format(concept, feature)\n",
    "    # prompt = \"Input-[Dolphin], [has_two_eyes] \\nOutput-Does a dolphine have two eyes?\\nInput-:[{}],[{}]\\nOutput-<mask>\".format(concept, feature)\n",
    "    prompt = \"Subject-:[{}]\\nPredicate[{}]\\nQuestion-<mask>\".format(concept, feature)\n",
    "    # prompt = \"{} {}?\".format(concept, feature)\n",
    "    batches_with_prompts.append([[concept, feature, prompt, 0]])\n",
    "    return\n",
    "\n",
    "def make_leuven_prompts(batches):\n",
    "    '''This function creates the prompts for the Leuven Norms experiment.'''\n",
    "    batches_with_prompts = []\n",
    "    Parallel(n_jobs=10, require='sharedmem')(delayed(add_leuven_prompt)(batch[0], batch[1], batches_with_prompts) for batch in batches)\n",
    "    return batches_with_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../data/leuven'\n",
    "animal_leuven_norms, artifacts_leuven_norms = load_leuven_norms(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "features = list(set(list(animal_leuven_norms.columns) + list(artifacts_leuven_norms.columns)))\n",
    "concepts = list(set(list(animal_leuven_norms.index) + list(artifacts_leuven_norms.index)))\n",
    "for concept, feature in itertools.product(concepts[:1], features[:1]):\n",
    "    batches.append([concept, feature])\n",
    "batches = make_leuven_prompts(batches)\n",
    "batches = np.array(list(itertools.chain(*batches)))\n",
    "prompts = batches[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "checker = pipeline(\"text-classification\", \"textattack/roberta-base-CoLA\")\n",
    "\n",
    "# Initialize the text-generation pipeline\n",
    "from transformers import pipeline\n",
    "corrector = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        \"pszemraj/flan-t5-large-grammar-synthesis\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "correcting text..: 100%|██████████| 1/1 [00:03<00:00,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "corrected_text = correct_text(prompts, checker, corrector)\n",
    "# pp.pprint(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So-and-so: [bo) Pleasure in _a_ and _ba_ as well as _c_ in a _ma_. Question and answer time  - i k ']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Input-[Dolphin], [has_two_eyes] \\nOutput-Does a dolphine have two eyes?\\nInput-:[bow],[used_as_a_draught_animal]\\nOutput-<mask>',\n",
       "       'Input-[Dolphin], [has_two_eyes] \\nOutput-Does a dolphine have two eyes?\\nInput-:[bow],[is_clothing]\\nOutput-<mask>',\n",
       "       'Input-[Dolphin], [has_two_eyes] \\nOutput-Does a dolphine have two eyes?\\nInput-:[bow],[crawls]\\nOutput-<mask>',\n",
       "       'Input-[Dolphin], [has_two_eyes] \\nOutput-Does a dolphine have two eyes?\\nInput-:[grenade],[used_as_a_draught_animal]\\nOutput-<mask>',\n",
       "       'Input-[Dolphin], [has_two_eyes] \\nOutput-Does a dolphine have two eyes?\\nInput-:[grenade],[is_clothing]\\nOutput-<mask>',\n",
       "       'Input-[Dolphin], [has_two_eyes] \\nOutput-Does a dolphine have two eyes?\\nInput-:[grenade],[crawls]\\nOutput-<mask>',\n",
       "       'Input-[Dolphin], [has_two_eyes] \\nOutput-Does a dolphine have two eyes?\\nInput-:[piranha],[used_as_a_draught_animal]\\nOutput-<mask>',\n",
       "       'Input-[Dolphin], [has_two_eyes] \\nOutput-Does a dolphine have two eyes?\\nInput-:[piranha],[is_clothing]\\nOutput-<mask>',\n",
       "       'Input-[Dolphin], [has_two_eyes] \\nOutput-Does a dolphine have two eyes?\\nInput-:[piranha],[crawls]\\nOutput-<mask>'],\n",
       "      dtype='<U128')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptual_represenations_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2886ae9a6130b79228c3d1879099aac127aa95ce688b2e915c2994996376314d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
