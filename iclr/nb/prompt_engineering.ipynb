{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddsuresh97/packages/anaconda3/envs/conceptual_represenations_gpt/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [03:08<00:00, 37.66s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\",  torch_dtype=torch.bfloat16,  cache_dir=\"/media/external/siddsuresh97/huggingface/\") \n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\", cache_dir=\"/media/external/siddsuresh97/huggingface/\")\n",
    "#/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Help me write a prompt as a question from a concept and an attribute. \\nConcept: monkey\\nAttribute: appears_in_comics.\\nPrompt: In one word Yes/No <mask> ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"In one word Yes/No, is the feature related to the concept.\\nConcept: {}\\Feature: {}.\\n <mask>\".format(\"monkey\", \"appears_in_comics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\"Q: Is the property [is_female] true for the concept [book]?\\nA: False\\nQ: Is the property [can_be_digital] true for the concept [book]?\\nA: True\\nIn one word True/False answer the following question:\\nQ: Is the property [{}] true for the concept [{}]?\\nA <mask>:\".format(\"monkey\", \"appears_in_comics\"), \"Q: Is the property [is_female] true for the concept [book]?\\nA: False\\nQ: Is the property [can_be_digital] true for the concept [book]?\\nA: True\\nIn one word True/False answer the following question:\\nQ: Is the property [{}] true for the concept [{}]?\\nA <mask>:\".format(\"monkey\", \"appears_in_comics\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Q: Is the property [is_female] true for the concept [book]?\\nA: FalseQ: Is the property [can_be_digital] true for the concept [book]?A: TrueIn one word True/False answer the following question:Q: : Is the property [has two eyes] true for the concept [dolphin]?A: <mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Does the dolphin have two eyes? <mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Help me write a question from the input.\\nInput - [Dolphin], [has_two_eyes]\\nQuestion - Do dolphins have_two_eyes?\\nInput - [piranha],[crawls]\\nQuestion -\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddsuresh97/packages/anaconda3/envs/conceptual_represenations_gpt/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Do piranhas eat crabs?</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['No', 1.0], ['No', 0.7]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=10, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=20\n",
    ")\n",
    "# print(\"Output:\\n\" + 100 * '-')\n",
    "answers = tokenizer.batch_decode(sample_outputs, skip_special_tokens=True)\n",
    "# replace True with Yes and False with No\n",
    "answers = [x.replace(\"True\", \"Yes\").replace(\"False\", \"No\") for x in answers]\n",
    "# group this in batches of 20\n",
    "answers = [answers[i:i+20] for i in range(0, len(answers), 20)]\n",
    "# # count number of times each answer appears, do this for each list\n",
    "answers = [[[x, answers[i].count(x)] for x in set(answers[i])]for i in range(len(answers))]\n",
    "# sort answers by count for each list\n",
    "answers = [[sorted(x, key=lambda x: x[1], reverse=True)] for x in answers]\n",
    "# store the top answer and percentage of times it appeared for each list\n",
    "answers = [[x[0][0][0], x[0][0][1]/20] for x in answers]\n",
    "answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['No', 15], ['Yes', 5]]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [sorted(x, key=lambda x: x[1], reverse=True) for x in answers[0]]\n",
    "x = answers[0]\n",
    "[sorted(x, key=lambda x: x[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 10747,     7,    15,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptual_represenations_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2886ae9a6130b79228c3d1879099aac127aa95ce688b2e915c2994996376314d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
