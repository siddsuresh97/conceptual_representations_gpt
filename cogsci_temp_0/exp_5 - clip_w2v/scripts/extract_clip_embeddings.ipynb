{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to make get_clip_embeddings work\n",
    "# import clip\n",
    "# import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    " \n",
    "# warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = ['Turtle',\n",
    " 'Alligator',\n",
    " 'Lizard',\n",
    " 'Tortoise',\n",
    " 'Cobra',\n",
    " 'Snake',\n",
    " 'Blindworm',\n",
    " 'Gecko',\n",
    " 'Boa Python',\n",
    " 'Toad',\n",
    " 'Crocodile',\n",
    " 'Chameleon',\n",
    " 'Caiman',\n",
    " 'Salamander',\n",
    " 'Dinosaur','Hammer',\n",
    " 'Screwdriver',\n",
    " 'Grinding Disc',\n",
    " 'Vacuum Cleaner',\n",
    " 'Spanner',\n",
    " 'Lawn Mower',\n",
    " 'Axe',\n",
    " 'Saw',\n",
    " 'Knife',\n",
    " 'Nail',\n",
    " 'Chisel',\n",
    " 'Shovel',\n",
    " 'Anvil',\n",
    " 'Oilcan',\n",
    " 'Paintbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ftfy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/siddharthsuresh/Projects/Wisconsin/repos/conceptual_representations_gpt/cogsci_temp_0/exp_5 - clip_embeddings/scripts/extract_clip_embeddings.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddharthsuresh/Projects/Wisconsin/repos/conceptual_representations_gpt/cogsci_temp_0/exp_5%20-%20clip_embeddings/scripts/extract_clip_embeddings.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddharthsuresh/Projects/Wisconsin/repos/conceptual_representations_gpt/cogsci_temp_0/exp_5%20-%20clip_embeddings/scripts/extract_clip_embeddings.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m lru_cache\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/siddharthsuresh/Projects/Wisconsin/repos/conceptual_representations_gpt/cogsci_temp_0/exp_5%20-%20clip_embeddings/scripts/extract_clip_embeddings.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mftfy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddharthsuresh/Projects/Wisconsin/repos/conceptual_representations_gpt/cogsci_temp_0/exp_5%20-%20clip_embeddings/scripts/extract_clip_embeddings.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mregex\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddharthsuresh/Projects/Wisconsin/repos/conceptual_representations_gpt/cogsci_temp_0/exp_5%20-%20clip_embeddings/scripts/extract_clip_embeddings.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m@lru_cache\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/siddharthsuresh/Projects/Wisconsin/repos/conceptual_representations_gpt/cogsci_temp_0/exp_5%20-%20clip_embeddings/scripts/extract_clip_embeddings.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbytes_to_unicode\u001b[39m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ftfy'"
     ]
    }
   ],
   "source": [
    "# helper function to get clip embeddings\n",
    "\n",
    "import gzip\n",
    "import html\n",
    "import os\n",
    "from functools import lru_cache\n",
    "\n",
    "import ftfy\n",
    "import regex as re\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = html.unescape(html.unescape(text))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
    "        merges = merges[1:49152-256-2+1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v+'' for v in vocab]\n",
    "        for merge in merges:\n",
    "            vocab.append(''.join(merge))\n",
    "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
    "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
    "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '',)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+''\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('', ' ')\n",
    "        return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get clip text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to extract the embeddings from the CLIP model given a list of concepts as text\n",
    "def get_clip_embeddings(texts):\n",
    "    # load the model\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda:0\")\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    text_tokens = [tokenizer.encode(\"This is \" + desc) for desc in texts]\n",
    "    text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
    "    sot_token = tokenizer.encoder['<|startoftext|>']\n",
    "    eot_token = tokenizer.encoder['<|endoftext|>']\n",
    "\n",
    "    for i, tokens in enumerate(text_tokens):\n",
    "        tokens = [sot_token] + tokens + [eot_token]\n",
    "        text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    text_input = text_input.cuda()\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_input).float()\n",
    "    # make a df with the embeddings and the text as row names\n",
    "    text_features = pd.DataFrame(text_features.cpu().numpy(), index=texts)\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_embeddings = get_clip_embeddings(concepts)\n",
    "# save the embeddings\n",
    "clip_embeddings.to_csv(\"../../data/clip/clip_embeddings.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec_embeddings():\n",
    "    # load the word2vec model\n",
    "    model = KeyedVectors.load_word2vec_format('/Users/siddharthsuresh/gensim-data/word2vec-google-news-300/word2vec-google-news-300', binary=True)\n",
    "    return model\n",
    "\n",
    "def get_word2vec_embeddings(model, word_list):\n",
    "    # replace 'Blindworm' with 'worm' as the word2vec model doesn't have the word 'Blindworm'\n",
    "    word_list = [word.replace('Blindworm', 'worm') for word in word_list]\n",
    "    word_list = [word.replace('Boa Python', 'python') for word in word_list]\n",
    "    word_list = [word.replace('Grinding Disc', 'cylindrical_grinding') for word in word_list]\n",
    "    word_list = [word.replace('Vacuum Cleaner', 'vacuum_cleaner') for word in word_list]\n",
    "    word_list = [word.replace('Lawn Mower', 'lawn_mower') for word in word_list]\n",
    "    # make sure all the words are lower case\n",
    "    word_list = [word.lower() for word in word_list]\n",
    "    word_list = [word.replace('axe', 'Axe') for word in word_list]\n",
    "    # get the word2vec representations of the words in the list\n",
    "    word_vectors = [model[word] for word in word_list]\n",
    "\n",
    "    # convert the list of word2vec representations to a numpy array\n",
    "    word_vectors = np.array(word_vectors)\n",
    "    # make a df with the embeddings and the word_list as row names\n",
    "    word_vectors = pd.DataFrame(word_vectors, index=word_list)\n",
    "    return word_vectors\n",
    "\n",
    "def get_similar_word2vec_embeddings(model, word):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        closest_word = model.most_similar(word, topn=1)[0][0]\n",
    "        return model[closest_word]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_word2vec_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embeddings = get_word2vec_embeddings(model, concepts)\n",
    "word2vec_embeddings.to_csv(\"../../data/word2vec/word2vec_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptual_representations_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4af0159948c59036d60f2810e081c8e724d4d7b9f72bddfb24f0ceb38cfd8de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
